{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12027baa-6e19-4597-84fe-b6e771747b71",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c97f9-a2e9-4290-a2f3-9d51346349a5",
   "metadata": {},
   "source": [
    "## Geometría analítica\n",
    "\n",
    "Antes de avanzar, recordemos nuestros cursos de Geometría Analítica: toda recta en el plano es una ecuación de la forma $$y=mx+b$$ donde \n",
    "\n",
    "* $b$ se conoce como *ordenada al origen* y es el valor sobre el eje Y en que en la recta lo atraviesa. En regresión lineal se le llama *intercepto*.\n",
    "\n",
    "* $m$ se conoce como *pendiente de la recta* y se identifica como la tangente inversa del ángulo que hace la recta con el eje X. En cristiano: $m$ mide la inclinación de la recta. También en regresión lineal se llama pendiente. Si $m>0$ la recta va hacia arriba; si $m<0$ la recta va hacia abajo; si $m=0$, la recta es horizontal.\n",
    "\n",
    "Observa la interacción del archivo **geogebra-recta_pendiente_origen.ggb** de nuestro repositoria [que mostramos aquí con GeoGebra](https://github.com/scidatmath2020/Inferencia-Estad-stica-2022/blob/main/geogebra-recta_pendiente_origen.ggb)\n",
    "\n",
    "Por lo tanto, **hallar la ecuación de una recta equivale a hallar los valores de $m$ y $b$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3a4e7-98b5-46b0-b284-75c490e0ba9d",
   "metadata": {},
   "source": [
    "# Regresiones lineales\n",
    "\n",
    "Las regresiones son maneras de *modelar* una característica medida utilizando otras, también medidas, de la misma muestra con el objetivo de crear predicciones. Esto es: si $X_1,X_2,...,X_n,X_{n+1}$ son algunas de las columnas de la tabla, encontrar una función $f$ tal que $X_{n+1}=f(X_1,X_2,...,X_n)$. En cristiano: **¿será posible explicar el comportamiento de una de las características a través del conocimiento de otras?**\n",
    "\n",
    "Bajo la idea anterior, decimos que las características $X_1,X_2,...,X_n$ son **explicativas** o **predictoras** y la característica $X_{n+1}$ es la **variable objetivo** o **a predecir**.  \n",
    "\n",
    "# Regresión lineal simple\n",
    "\n",
    "En esta sección platicaremos de un problema de regresión muy sencillo conocido como **regresión lineal**. Observemos la siguiente nube de puntos: \n",
    "\n",
    "<img src=\"im022.png\" style=\"display:block; margin:auto;\">\n",
    "\n",
    "Debido a su forma, vale preguntarse cuál será la recta que mejor se aproxime, en algún sentido, a todos los puntos al mismo tiempo.\n",
    "\n",
    "Observemos varias rectas graficadas con la nube de puntos. ¿Cuál dirías que es la que más se *ajusta* a todos los puntos al mismo tiempo?\n",
    "\n",
    "<img src=\"im023.png\" style=\"display:block; margin:auto;\">\n",
    "\n",
    "De esta manera, sean $X$ y $Y$ dos características de tu población. Decimos que el modelo que explica a $Y$ a través de $X$ es lineal si tenemos razones para pensar que existen números $\\beta_0$ y $\\beta_1$ tales que $Y=\\beta_0+\\beta_1X+\\varepsilon$ donde $\\varepsilon$ es una variable aleatoria gaussiana con media 0 (un ruido blanco)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f63b3a-8e1e-469e-a957-2854d24598c3",
   "metadata": {},
   "source": [
    "## Mínimos cuadrados\n",
    "\n",
    "Recordemos nuestro objetivo: tenemos una lista de parejas de puntos $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$. Los graficamos como nube de puntos y buscamos la recta que mejor aproxime, en cierto sentido, a todos los puntos al mismo tiempo. \n",
    "\n",
    "En principio, cualquier recta $y=mx+b$ es una recta que podemos tomar como aproximación. Por lo tanto, a cada $x_i$ se le asignan dos números: el $y_i$ (que es un valor que conocemos) y el $\\hat{y_i}=mx_i+b$, que es el valor que nos da la recta para ese número $x_i$.\n",
    "\n",
    "* **Predicciones.** Son los valores $\\hat{y_1},\\hat{y_2},...,\\hat{y_n}$. Es decir, los valores que *la recta predice*.\n",
    "\n",
    "* **Residuos.** ¿Qué tanto se equivocó la recta? Recordemos: la recta le asigna a $x_i$ el valor $\\hat{y_i}$. Pero el valor verdadero que acompaña a $x_i$ es $y_i$. Los residuos son los errores que la recta cometió: $\\varepsilon_i=y_i-\\hat{y_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c8daf-0227-4ad3-9634-dfd7038a6627",
   "metadata": {},
   "source": [
    "Por lo tanto tenemos el siguiente resultado:\n",
    "\n",
    "Si $(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)$ son una muestra de tamaño $n$ de la característica $(X,Y)$ (es decir, $n$ renglones de tu tabla tomando solo las columnas $X$ y $Y$), entonces los estimadores para $\\beta_0$ y $\\beta_1$ de la recta de mínimos cuadrados son $$b_1=\\frac{\\sum(X_i-\\overline{X})(Y_i-\\overline{Y})}{\\sum(X_i-\\overline{X})^2}\\,\\,\\mbox{ y \n",
    "}\\,\\,b_0=\\overline{Y}-b_1\\overline{X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8796890-c302-4e06-b3e3-545d94c2dfde",
   "metadata": {},
   "source": [
    "## Tipos de anomalías en regresiones lineales\n",
    "\n",
    "Las observaciones anómalas pueden provocar que se malinterpreten patrones en el conjunto de datos. Además, puntos aislados pueden tener una gran influencia en el modelo de regresión, dando resultados completamente diferentes. Por ejemplo, pueden provocar que nuestro modelo no capture características importantes de los datos. \n",
    "\n",
    "Por ello, es importante detectarlas.\n",
    "\n",
    "Existen tres tipos de observaciones anómalas:\n",
    "\n",
    "* **Leverages.** son observaciones con un valor anómalo de las variables de control. No tienen por qué afectar los coeficientes de la regresión.\n",
    "\n",
    "* **Outliers de regresión** son observaciones que tienen un valor anómalo de la variable $Y$, condicionado a los valores de sus variables independientes $X_i$. Tendrán un residuo muy alto pero no pueden afectar demasiado a los coeficientes de la regresión.\n",
    "\n",
    "* **Observaciones influyentes** son aquellas que tienen un leverage alto; son outliers de regresión y afectan fuertemente a la regresión.\n",
    "\n",
    "<img src=\"im024.png\" style=\"display:block; margin:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28372c61-97b0-4cc1-848f-31330cb0f203",
   "metadata": {},
   "source": [
    "## Regresión lineal con puntos influyentes\n",
    "\n",
    "Como ya dijimos, estos los puntos influyentes son precisamente los renglones de la tabla que pueden cambiar drásticamente la regresión lineal. Para ejemplificarlo, utilizaremos la tabla **cisne.csv** de nuestro repositorio de datos.\n",
    "\n",
    "En general, existen varios métodos de regresión lineal robusta. En esta sección estudiaremos tres de ellos.\n",
    "\n",
    "Recordemos que el problema de Regresión lineal usual consiste en minimizar $$\\sum_{i=1}^n\\varepsilon_i^2$$\n",
    "\n",
    "### Desviación absoluta mínima: LAD\n",
    "\n",
    "Consiste en minimizar $$\\sum_{i=1}^n|\\varepsilon_i|$$\n",
    "\n",
    "### Mínimos cuadrados medianos: LMS\n",
    "\n",
    "Consiste en minimizar $$mediana(\\varepsilon_i^2)$$\n",
    "\n",
    "### Mínimos cuadrados recortados: LTS\n",
    "\n",
    "Consiste en minimizar $$\\sum_{i=1}^h(\\varepsilon_i)^2$$\n",
    "\n",
    "donde $h=n/2$ y los errores se ordenan de menor a mayor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2ea57-0180-43b3-a66f-861b7e874c98",
   "metadata": {},
   "source": [
    "### Implementación en R y Python.\n",
    "\n",
    "Puedes revisar los scripts reg_lineal_robusta de nuestro repositorio de auxiliares: https://github.com/scidatmath2020/An_rob_2025/tree/main/auxiliares"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
